\documentclass[fontsize=12pt, DIV=10]{scrreprt}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{fontspec}
\usepackage{mathpazo}
\setmainfont
     [ BoldFont       = texgyrepagella-bold.otf ,
       ItalicFont     = texgyrepagella-italic.otf ,
       BoldItalicFont = texgyrepagella-bolditalic.otf ]
     {texgyrepagella-regular.otf}
     
\usepackage{mathpazo}

% Setting up referencing
\usepackage[backend=biber]{biblatex}
\addbibresource{references.bib}

% Setting up theorem, claim, proof, etc. environments
\usepackage{thmtools}
\usepackage{thm-restate}

\usepackage{varioref}
\usepackage[hidelinks]{hyperref}
\usepackage{cleveref}

\usepackage{mdframed}

\declaretheorem[name=Definition, numberwithin=chapter, thmbox=M]{defn}
\declaretheorem[name=Theorem, thmbox=M, sibling=defn]{theorem}
\declaretheorem[name=Lemma, thmbox=M]{lemma}
\declaretheorem[
	name=Proposition,
	refname={proposition, propositions},
	Refname={Proposition, Propositions},
	thmbox=M,
	sibling=defn]{prop}
\declaretheorem[name=Corollary, thmbox=M, sibling=defn]{corollary}

\usepackage{amsthm}

\newtheorem{claim}{Claim}

\theoremstyle{remark}
\newtheorem{remark}{Remark}

\newenvironment{subproof}[1][\proofname]{%
  \renewcommand{\qedsymbol}{$\blacksquare$}%
  \begin{proof}[#1]%
}{%
  \end{proof}%
}

\newcommand{\defeq}{\coloneqq}
\let\C\relax
\newcommand{\R}{\mathbb R}
\newcommand{\C}{\mathbb C}
\newcommand{\E}{\mathbb E}
\newcommand{\N}{\mathbb N}
\newcommand{\prob}{\mathbb P}
\newcommand{\indi}{\mathbb 1}
\newcommand{\calA}{\mathcal A}
\newcommand{\calB}{\mathcal B}
\newcommand{\calD}{\mathcal D}
\newcommand{\calE}{\mathcal E}
\newcommand{\calF}{\mathcal F}
\newcommand{\calH}{\mathcal H}
\newcommand{\dif}[1]{\text{d} #1}
\newcommand{\towk}{\Rightarrow}
\DeclareMathOperator{\reg}{Reg}
\DeclareMathOperator{\var}{Var}
\DeclareMathOperator{\cov}{Cov}
\DeclareMathOperator{\spn}{span}
\DeclareMathOperator{\tr}{Tr}
\DeclarePairedDelimiterX{\abs}[1]{\lvert}{\rvert}{#1}
\DeclarePairedDelimiterX{\norm}[1]{\lVert}{\rVert}{#1}
\DeclarePairedDelimiterX{\inn}[2]{\langle}{\rangle}{#1, #2}
\newcommand{\boxspace}{\ensuremath{\R^{[0, 1]^d}}}
\newcommand{\unitbox}{\ensuremath{[0,1]^d}}
\renewcommand{\phi}{\varphi}

\title{Stochastic Partial Differential Equations}
\author{Based on Lectures by Dr. A. Chandra and Dr. G. Cannizzaro}

\begin{document}

\maketitle

\tableofcontents

\chapter{Gaussian Measures on Banach Spaces}

\section{Introduction}

Throughout these notes let $\calB$ be a real Banach space. Usually $\calB$ will also be seperable, but we'll say explicitly when this is the case. Let $\mu$ be a Borel probability measure on $\calB$ and $X$ be a $\calB$-valued random variable such that $X \sim \mu$. We'd like recover our finite dimensional tools for studying random variables. Let's start with the mean. From the $\R^n$-case, it's tempting to define the mean as
\begin{equation}
	m \defeq \int_{\calB} x \, \mu(\dif x).
\end{equation}
However this integral cannot be understand as a Lebesgue integral. After all, $x$ doesn't take values in $\R^n$. At this point we have two options.

\subsection{Option One: The Bochner Integral}

We could try and rigorously define this integral. The method of integration used for $\calB$-valued functions are Bochner integrals. The construction of the Bochner integral follows the same method as the Lebesgue integral; we define the integral for a class of simple functions and extend the integral by approximation. Rather than going into details on the construction, we'll list off some of the properties of the Bochner integral. Firstly let $(E, \calE, \nu)$ be a probability space and $f: E \to \calB$ be measurable. Then $f$ is integrable if and only if
\begin{equation}
	\int_E \norm{f(x)}_E \, \nu(\dif x) < \infty.
\end{equation}
The Bochner integral also behaves nicely with bounded linear operators. Let $\hat{\calB}$ be another Banach space and $L: \calB \to \hat{\calB}$ be a bounded linear operator. Then the Bochner integral can be exchanged with $L$, i.e.\ for all integrable $f: E \to \calB$,
\begin{equation}
	\int_E L(f(x)) \, \nu(\dif x) = L\left( \int_E f(x) \, \nu(\dif x)\right).
\end{equation}
Hence if we assume $\int_E \norm{x} \, \mu(\dif x) < \infty$ then the mean
\begin{equation}
	\bar{m}_{\mu} \defeq \int_E x \, \mu(\dif x)
\end{equation}
can be defined using the Bochner integral. This approach is valuable in how it carrys the intuition from the finite dimensional case. In addition, it makes clear that the mean is an element of $\calB$ without the use of reflexivity (as we will see in the next approach). 

\subsection{Option Two: One-Dimensional Marginals}

Alternatively, we've already developed a rich theory of Lebesgue integration for $\R$-valued functions. What if we instead used this theory? This would require a `natural' way to map $\calB$ to $\R$. Since $\calB$ is a normed vector space, we have exactly that: the space of bounded linear functionals $\calB^*$. For any $l \in \calB^*$, $\E[l(X)]$ is just given by a Lebesgue integral. So to define the mean, just gather all these values for diffent $l \in \calB^*$. Following this idea, we define the mean as $m: \calB^* \to \R$ where
\begin{equation}
	m(l) \defeq \E[l(X)] = \int_{\calB} l(x) \, \mu(\dif x).
\end{equation}
How can we recover a mean $\bar{m}_{\mu} \in \calB$? Suppose that $\int_E \norm{x} \, \mu(\dif x) < \infty$. We can then prove that $m_{\mu}: \calB^* \to \R$ is a bounded linear functional, in other words $m_{\mu} \in \calB^{**}$. At this point we could assume that $\calB$ were reflexive. Then we are guaranteed there exists $\bar{m}_{\mu} \in \calB$ such that
\begin{equation}
	m_{\mu}(l) = l(\bar{m}_{\mu}) \quad \forall l \in \calB^*.
\end{equation}
However, this is unnecessary. From our first approach of using Bochner integrals, we know in fact that $\int_{\mu} \norm{x} \, \mu(\dif x) < \infty$ is a sufficient condition to define $\bar{m}_{\mu}$ as in. Then since the Bochner integral can be exchanged with any $l \in \calB^*$, the property in holds. This shows two things. Firstly, it shows our two approaches recover the same mean. Secondly, it shows reflexivity is an extraneous assumption.

The value of this second approach really shines when the concept we're describing becomes more intangible than a mean, for example when defining Gaussianity on a Banach space.

\section{Random Variables on Banach Spaces}


Seen as we're working with $l(X)$ so much, it's useful to have a notation for its law.
\begin{defn}
	Let $(E, \calE)$ and $(F, \calF)$ be measurable spaces. Let $\mu$ be a measure on $(E, \calE)$ and $f: (E, \calE) \to (F, \calF)$ be measurable. Then the \emph{pushforward} of $\mu$ be $f$ is the measure $f_* \mu$ on $(F, \calF)$ where
	\begin{equation}
		f_* \mu(A) \defeq \mu(f^{-1}(A)).
	\end{equation}
\end{defn}

This is defined so that $l(X) \sim l_* \mu$. It is also sensible to have a name for the laws $\{l_* \mu : l \in \calB^*\}$.
\begin{defn}
	Let $\mu$ be a probability measure on $\calB$. Then the \emph{one-dimenional margins} of $\mu$ are the laws $l_* \mu$ for all $l \in \calB^*$.
\end{defn}

The next proposition shows that the one-dimensional marginals carry enough information to uniquely characterise a meaure $\mu$.
\begin{prop}
	\label{prop:one-dim-marginals}
	Let $\mu$ and $\nu$ be Borel probability measures on a seperable Banach space $\calB$. Suppose $l_* \mu = l_* \nu$ for all $l \in \calB^*$. Then $\mu = \nu$.
\end{prop}
\begin{proof}
	INSERT PROOF.
\end{proof}

Next we formalise what we said in the introduction.

\begin{defn}
	Let $\mu$ be a probability measure on $\calB$ and suppose $l_* \mu$ has finite mean for all $l \in \calB^*$. Then the \emph{mean operator} of $\mu$ is a linear operator $\mu: \calB^* \to \R$ where
	\begin{equation}
		m_{\mu}(l) \defeq \int_{\cal B} l(x) \, \mu(\dif x).
	\end{equation}
	$\mu$ is \emph{centered} if $m_{\mu} \equiv 0$.
\end{defn}

Similarly we can define the covariance.

\begin{defn}
	Let $\mu$ be a probability measure on $\calB$ and suppose $l_* \mu$ has finite second moment for all $l \in \calB^*$. Then the \emph{covariance operator} of $\mu$ is the bilinear operator $C_{\mu} : \calB^* \times \calB^* \to \R$ where
	\begin{equation}
		C_{\mu}(l, l') \defeq \int_{\calB} l(x) l'(x) \mu(\dif x).
	\end{equation}
\end{defn}
\begin{remark}
	Note that $C_{\mu}$ is a symmetric, positive semi-definite, bilinear operator.
\end{remark}

In the next section we'll show these operators are bounded for a Gaussian operator. Finally we'll define the Fourier transform.

\begin{defn}
	The \emph{Fourier transform} of a probabilty measure $\mu$ on $\calB$ is given by $\hat{\mu} \in \calB^* \to \C$ where
	\begin{equation}
		\hat{\mu}(l) = \int_{\calB} e^{i l(x)} \mu(\dif x).
	\end{equation}
\end{defn}
\begin{remark}
	Unlike the mean, the Fourier transform of a measure of $\R^n$ is normally a function to begin with, just a function $\R^n \to \R$ rather than $(\R^n)^* \to \R$. This is a use of the Hilbert space structure on $\R^n$. For a general Hilbert space $\cal H$, let $\mu$ be a probabilty measure on $\calH$. We can then instead define $\hat{\mu}: \calH \to \R$ by
	\begin{equation}
		\hat{\mu}(\xi) \defeq \int_{\calH} e^{i \inn{\xi}{x}} \mu(\dif x).
	\end{equation}
	This is then equivalent to our definition since by the Riesz representation theorem the map $\xi \to l_{\xi}$ where $l_{\xi}(x) = \inn{\xi}{x}$ is an isomorphism between $\calH$ and $(\calH)^*$.
\end{remark}

In the finite dimensional case, Fourier transforms uniquely determine distributions. This holds true in seperable Banach spaces.
\begin{prop}
	Let $\mu$ and $\nu$ be probability measures on $\calB$. If $\hat{\mu} = \hat{\nu}$ then $\mu = \nu$.
\end{prop}
\begin{proof}
	$\hat{\mu}$ uniquely determines the (classical) Fourier transform of $l_* \mu$ since
	\begin{equation}
		\widehat{l_* \mu}(\xi) = \hat{\mu}(\xi l).
	\end{equation}
	From the finite dimensional case we know $\widehat{l_* \mu}$ uniquely determines $l_* \mu$. Therefore $\hat{\mu}$ uniquely determines the one-dimensional marginals of $\mu$, and thus $\mu$ itself by \vref{prop:one-dim-marginals}.
\end{proof}

Now we've got our tools ready, we start focusing specifically on Gaussian measures.

\section{Basic definition of Gaussian Measures}

We start by defining what a Gaussian measure is. As aluded to in the introduction, there is not an immediately obvious definition for what a Gaussian measure is on $\calB$. However we do know what Gaussian measures are on $\R$, so we can use our approach of considering one-dimensional marginals.
\begin{defn}
	A Borel probability measure $\mu$ on $\calB$ is a \emph{Gaussian measure} if $l_* \mu$ is a Gaussian measure \footnote{We count Dirac measures as Gaussian measures with zero variance} on $\R$ for all $l \in \calB^*$.
\end{defn}

Gaussian measures on $\R^n$ are uniquely characterised by their covariance and mean. This is true since they uniquely determine the Fourier transform. This remains true in infinite dimensions.
\begin{prop}
	Let $\mu$ be a Gaussian measure on $\calB$. Then
	\begin{equation}
		\hat{\mu}(l) = \exp\left\{i m_{\mu}(l) - \frac{1}{2} C_{\mu}(l, l) \right\}.
	\end{equation}
\end{prop}
\begin{corollary}
	A Gaussian measure on a seperable Banach space is uniquely determined by it's mean and covariance operators.
\end{corollary}
We've already stated that covariance operators need to be positive definite, symmetric and bilinear. But is the converse true, can we get an existence result? Speficially given such an operator, can we necessarily find a centered Gaussian measure with that covariance? Unfortunately no, and we'll see one reason why in the next section.

\section{Integrability of Gaussian Measures}

We now show decay properties of Gaussian measures. Suprisingly, this will come from rotational invariance of Gaussian measure. 

\begin{defn}
	Let $\mu$ be a measure on $\calB$. For each $\phi \in \R$, define $R_{\phi}: \calB \times \calB \to \calB \times \calB$ by
	\begin{equation}
		R_{\phi}(x, y) = (x \sin \phi + y \cos \phi, x \cos \phi - y \sin \phi).
	\end{equation}
	Then $\mu$ is \emph{invariant under rotations of $\phi$} if $(R_{\phi})_* (\mu \otimes \mu) = \mu \otimes \mu$.
\end{defn}

\begin{prop}
	Centered Gaussian measures on a seperable Banach space $\calB$ are invariant under rotations by any angle.
\end{prop}
\begin{proof}
	Check the Fourier transform of the rotated measure is equal to the fourier transform of the original measure.
\end{proof}

The following theorem shows as long as a measure is invariance under rotations of $\frac{\pi}{4}$ then it has double exponential tail decay.
\begin{theorem}[Fernique]
	Let $\mu$ a probability measure on $\calB$. Suppose $\mu$ is invariant under rotations by $\frac{\pi}{4}$. Then there exists $\alpha > 0$ such that $\norm{x}$ has doubly exponential moments, i.e.\
	\begin{equation}
		\int e^{\alpha \norm{x}^2} \, \mu(\dif x) < \infty.
	\end{equation}
\end{theorem}
\begin{proof}
	Take $t, \tau > 0$. Then
	\begin{align}
		\mu(\norm{x} \leq \tau) \mu(\norm{x} > t)
		&= \mu \otimes \mu\left( \frac{\norm{x + y}}{\sqrt{2}} > t, \frac{\norm{x+y}}{\sqrt{2}} > \tau \right) \\
		&\leq \mu \otimes \mu \left(\norm{x} > \frac{t - \tau}{2}, \norm{x} > \frac{t - \tau}{2}) \right) \\
		&= \mu\left(\norm{x} > \frac{t - \tau}{2}\right)^2
	\end{align}

	INSERT REST OF PROOF
\end{proof}

Fernique's theorem allows us to not only show all moments of a Gaussian measure are finite, it allows us to bound the higher moments using the first moment.

\begin{prop}
	Let $\mu$ be a centered Gaussian on $\calB$. Then there exists $\alpha, K > 0$ such that for all $n \in \N$
	\begin{equation}
		\int_{\calB} \abs{x}^{2n} \, \mu(\dif x) \leq n! K \alpha^{-n} M^{2n}.
	\end{equation}
	where $M = \int_{\calB} \norm{x} \, \mu(\dif x)$.
\end{prop}
\begin{proof}
	INSERT PROOF
\end{proof}

Let's return to our question of showing existence of a Gaussian measure given a covariance operator. We already know they must be symmetric and PSD. The following theorem shows that they must further be bounded operators.
\begin{prop}
	\label{prop:bounded-cov}
	Let $\mu$ be a centered Gaussian measure on a seperable Banach space. Then there exists $\norm{C_{\mu}} < \infty$ such that $C_{\mu}(l, l') \leq \norm{C_{\mu}} \norm{l} \norm{l'}$. Moreover there exists continuous operator $\hat{C}_{\mu}: \calB^* \to \calB$ such that
	\begin{equation}
		C_{\mu}(l, l') = l'\left(C_{\mu}(l) \right).
	\end{equation}
\end{prop}
\begin{proof}
	The first part of the proof is clear by setting $\norm{C_{\mu}}$ to $\int_{\calB} \norm{x}^2 \, \mu(\dif x)$ and employing boundedness of $l$ and $l'$.

	For the second part, since
	\begin{equation}
		\int_{\calB} \norm{x l(x)} \, \mu(\dif x) \leq \norm{l} \int_{\calB} \norm{x}^2 \dif x < \infty
	\end{equation}
	we can formalise
	\begin{equation}
		\hat{C}_{\mu}(l) = \int_{\calB} x l(x) \, \mu(\dif x)
	\end{equation}
	as a Bochner integral. Then since the Bochner integral can be exchanged with continuous linear functionals
	\begin{align}
		l'(\hat{C}_{\mu}(l))
		&= l' \left( \int_{\calB} x l(x) \, \mu(\dif x)\right)
		= \int_{\calB} l'(x l(x)) \, \mu(\dif x) \nonumber \\
		&= \int_{\calB} l(x) l'(x) \mu(\dif x)
		= C_{\mu}(l, l')
	\end{align}
	as required.
\end{proof}
So now let's add boundedness to our list of assumptions. Is that enough to characterize covariance operators of Gaussian measures? Unfortunately the answer is still no. In fact for a general Banach space there is no nice characterisation. However for Hilbert spaces we can give an actual answer.

\section{Characterisation of Gaussian measures on Hilbert spaces}

As promised, we now characterise the covariance operators  of Gaussian measures. In \vref{prop:bounded-cov} we've already shown there exists $\hat{C}_{\mu}: \calH^* \to \calH$ such that $C_{\mu}(l, l') = l'(\hat{C}_{\mu}(l))$ for all $l, l' \in \calB^*$. Then by the Riesz representation, there exists $\tilde{C}_{\mu}: \calH \to \calH$ such that
\begin{equation}
	C_{\mu}(\inn{\cdot}{x}, \inn{\cdot}{y}) = \inn{\tilde{C}_{\mu}x}{y}
\end{equation}
We will state our characterisation in terms of $\tilde{C}_{\mu}$. Firstly let's recall some definitions from functional analysis.
\begin{defn}
	Let $\calH$ be a seperable Hilbert space and let $L: \calH \to \calH$ be linear.
	\begin{enumerate}
		\item $L$ is \emph{positive definite} if $\inn{L h}{h} > 0$ for all non-zero $h \in \calH$
		\item $L$ is \emph{trace-class} if for any\footnote{You can show that as long as this condition holds for one orthonomal basis, it must hold for all orthonormal bases} orthonormal basis $(e_n)$ of $\calH$ there exists a sequence of numbers $\{\lambda_n\}$ such that $\sum_n \abs{\lambda_n} < \infty$ and
			\begin{equation}
				\inn{x}{Lx} = \sum_n \lambda_n \inn{x}{e_n}^2.
			\end{equation}
			The \emph{trace} of $L$ is then $\tr L \defeq \sum_n \lambda _n$.
	\end{enumerate}
\end{defn}

Trace-class is a stronger property than boundedness, in particular there exists operators that are bounded and not trace class. Hence we see symmetry, positive semi-definitess, and boundedess was truly insufficient to characterize Gaussian covariances.
\begin{prop}
	Let $\calH$ be a seperable Hilbert space and $\tilde{C}: \calH \to \calH$. Then the following are equivalent
	\begin{enumerate}
		\item There exists a Gaussian measure $\mu$ on $\calH$ such that
			\begin{equation}
				C_{\mu}(\inn{\cdot}{x}, \inn{\cdot}{y}) = \inn{\tilde{C} x}{y} \quad \forall x, y \in \calH.
			\end{equation}
		\item $\tilde{C}$ is a linear, positive semi-definite, trace class operator
	\end{enumerate}
\end{prop}
\begin{proof}
	INSERT PROOF
\end{proof}

\section{Regularity of Gaussian measures}

This section focuses on Gaussian measures on $C([0,1]^d, \R)$ under the supremum norm. Hence it makes sense to talk about the regularity of the paths of a Gaussian measure.

The following is both an existence result about Gaussian measures with sufficiently smooth paths.
\begin{theorem}[Kolmogorov Continuity Criterion]
	\label{thm:kcc}
	For $d \geq 1$, let $C: [0, 1]^d \times [0, 1]^d$ be a function\footnote{this is not a bilinear operator} such that:
	\begin{enumerate}
		\item For any finite collection of points $\{x_1, \ldots, x_n\} \subset [0, 1]^d$ the matrix
			\begin{equation}
				C_{ij} \defeq C(x_i, x_j)
			\end{equation}
			is symmetric, positive semi-definite.
		\item There exists $\alpha > 0$ and $K > 0$ such that
			\begin{equation}
				C(x, x) - 2C(x, y) + C(y, y) \leq K \abs{x - y}^{2 \alpha}.
			\end{equation}
	\end{enumerate}
	Then there exists a unique Gaussian measure $\mu$ on $C([0, 1]^d, \R)$ such that
	\begin{equation}
		C_{\mu}(\delta_x, \delta_y) \defeq \int_{C([0, 1]^d, \R)} f(x) f(y) \, \mu(\dif f) = C(x, y)
	\end{equation}
	for all $x, y \in [0, 1]^d$ and $\mu(C^{\beta}([0, 1]^d, \R)) = 1$ for all $\beta \in (0, \alpha)$.
\end{theorem}

Before we prove it, let's understand this theorem and give some intuition to the proof. Firstly, note $C$ is a function $[0,1]^d \times [0, 1]^d \to \R$ rather than $C([0,1]^d, \R)^* \times C([0, 1]^d, \R)^* \to \R$. This can be understand as specifying the covariance structure of only $\{(\delta_x)_* \mu : x \in [0, 1]^d\}$ rather than for all one-dimensional marginals. We'll first show this is enough to characterise and ensure Gaussianity of $\mu$.

\begin{prop}
	\label{prop:weak-star-suff}
	Let $\mu$ be a Borel probability measure on $\calB$ and let $\calA$ be weak-* dense in $\calB^*$. 
	\begin{enumerate}
		\item If $l_* \mu$ is Gaussian for all $l \in \calA$ then $\mu$ is Gaussian.
		\item If $\calB$ is seperable then $\{l_* \mu : l \in \calB^*\}$ uniquely determines $\mu$.
	\end{enumerate}
\end{prop}
\begin{proof}
	Given any $l \in \calB^*$, take a sequence $l_n \in \calA$ such that $l_n \xrightharpoonup{w*} l$ as $n \to \infty$. Fix any $f: \R \to \R$ bounded and continuous, the by definition of weak-$*$ convergence $f(l_n(x)) \to f(l(x))$ as $n \to \infty$ for all $x \in \calB$. Therefore by the dominated convergence theorem
	\begin{equation}
		\int_{\calB} f(l_n(x)) \, \mu(\dif x) = \lim_{n \to \infty} \int_{\calB} f(l(x)) \, \mu(\dif x).
	\end{equation}
	Therefore $(l_n)_* \mu \towk l \mu$ as $n \to \infty$. Since the weak limit of Gaussians on $\R$ is Gaussian, $l_* \mu$ is Gaussian. Thus $\mu$ is a Gaussian measure.


	The uniqueness statement follows from \vref{prop:one-dim-marginals} since the one-dimensional marginals are uniquely determined as weak limits of sequences in $\{l_* \mu : l \in \calA\}$.
\end{proof}

Thus we need linear combinations of Dirac measures to be weak-$*$ dense. This is true, and we'll shove the proof into the appendix.
\begin{restatable}{prop}{dirac}
	\label{prop:dirac-density}
	Suppose that $\calD \subset \unitbox$ is dense in $\unitbox$. Then 
	\begin{equation}
		\calA \defeq \spn\{\delta_x : x \in \calD\}
	\end{equation}
	is weak-$*$ dense in $C(\unitbox, \R)^*$.
\end{restatable}

Secondly, let's understand the continuity statement better. Suppose we've already proven there exists a Gaussian measure $\mu$ on $C([0, 1]^d, \R)$ such that $C(x, y) = C_{\mu}(\delta_x, \delta_y)$ for all $x, y \in [0, 1]^d$. Rather than working directly with $\mu$, it's more intuitive to work with a continuous random process $F: \Omega \to C([0, 1]^d, \R)$ that has law $\mu$. We'll use the notation $F_x(\omega) \defeq F(\omega)(x)$. Then $C(x, y) = C_{\mu}(\delta_x, \delta_y) = \text{Cov}(F_x, F_y)$. Thus the condition (2) on $C$ is equivalent to requiring
\begin{equation}
	\norm{F_x - F_y}_2 \defeq \sqrt{\E[(F_y - F_y)^2]} \leq K \abs{x - y}^{\alpha} \quad \forall x, y \in [0, 1]^d.
\end{equation}
So condition (2) gives $\alpha$-H\"older continuity of the map $[0, 1]^d \to L^2(\Omega)$ where $x \mapsto F_x$. We'd like this to instead be a pathwise statement, for fixed $\omega$ is the map $x \mapsto F_x(\omega)$ H\"older continuous? While we can't get $\alpha$-H\"older continuity, the above theorem says for we can arbitary close. In particular for almost every $\omega$, $x \mapsto F_x(\omega)$ is $\beta$-H\"older continuous for all $\beta \in (0, \alpha)$.

It should be noted that condition (2) isn't only used to show pathwise H\"older continuity, it is used to ensure there even exists a Gaussian measure $\mu$ on  the space $C([0, 1]^d, \R)$ such that $C_{\mu}(\delta_x, \delta_y) = C(x, y)$ in the first place. We know Gaussian covariances are at least continuous so some continuity criterion is certainly needed. Maybe if we didn't need H\"older continuity we can give a weaker condition, but something needs to replace condition (2) to ensure existence.

\begin{proof}
	By Kolmogorov's extension theorem, there exists a stochastic process $F: \Omega \to \boxspace$ such that for all $x_1, \ldots, x_n \in [0, 1]^d$, 
	\begin{equation}
		(F_{x_1}, \ldots, F_{x_n})^T
		\sim N(0, \Sigma)
	\end{equation}
	where $\Sigma_{i,j} = C(x_i, x_j)$. Condition (1) ensures that $\Sigma$ is a valid covariance matrix. However Kolmogorov's extension theorem only gives that $F: \Omega \to \boxspace$ is measurable with respect to the product $\sigma$-algebra from $\boxspace$. This is a very weak $\sigma$-algebra, meaning we have to be careful about what events we consider.

	We want to determine H\"older continuity of $F_{\cdot}(\omega)$. Fix $\beta \in (0, \alpha)$. Then $F_{\cdot}$ would be $\beta$-H\"older continuous if
	\begin{equation}
		\sup_{x, y \in [0, 1]^d, x \neq y} \frac{\abs{f(x) - f(y)}}{\norm{x - y}^{\beta}} < \infty.
	\end{equation}
	However the quantity on the left is not a measurable function of $F$, so we approximate it. Let $\calD$ be the grid of dyadic points in $[0, 1]^d$. Note $\calD$ is countable and dense in $[0, 1]^d$. We consider the approximation $M_{\beta}(f): \boxspace \to \R$ by
	\begin{equation}
		M_{\beta}(f) = \sup_{x, y \in \calD, x \neq y} \frac{\abs{f(x) - f(y)}}{\norm{x - y}^{\beta}}.
	\end{equation}
	The function inside the supremum is measurable with respect to the product $\sigma$-algebra since it is a pointwise evaluation. Then $M_{\beta}$ is measurable with respect to the product $\sigma$-algebra as a countable supremum of measurable functions. If $M_{\beta}(f) < \infty$ then $f$ is $\beta$-H\"older continuous along the dyadic rationals, we want to extend this to the entirety of $f$. To assist with this we can define the regulariser $\reg_{\beta}: \boxspace \to C([0, 1], \R)$ by
	\begin{equation}
		\reg_{\beta}(f)(x) \defeq 
		\begin{cases}
			\lim_{s \in \calD, s \to x} f(s) & \text{if}\ M_{\beta}(f) < \infty \\
			0 & \text{otherwise}.
		\end{cases}
	\end{equation}
	\newcommand{\regindi}{\ensuremath{\indi_{\{M_{\beta}(f) < \infty\}}}}
	\begin{claim}
		$\reg_{\beta}: \boxspace \to C([0, 1]^d, \R)$ is well defined (in particular the limit in the definition exists and is independent of the sequence taken) and 
		\begin{enumerate}
			\item For all $f \in \boxspace$, $\reg_{\beta}(f) \in C^{\beta}([0, 1]^d, \R)$
			\item $\reg_{\beta}(f)(x) = f(x) \regindi$ for all $x \in \calD$
			\item $\reg_{\beta}$ is measurable with respect to the product $\sigma$-algebra on $\boxspace$ and the Borel $\sigma$-algebra on $C([0, 1]^d, \R)$ (under supremum norm).
		\end{enumerate}
	\end{claim}
	\begin{subproof}
	We'll only show (3) here. Since the Borel algebra on $C([0, 1]^d, \R)$ is generated by balls, it suffices to check that $\{f \in \boxspace : \norm{\reg_{\beta}(f) - f_0}_{\infty} < r\}$ is measurable for all $f_0 \in C([0, 1]^d, \R)$ and $r > 0$. By continuity of $\reg_{\beta}(f)$ and density of $\calD$,
	\begin{align}
		\norm{\reg_{\beta}(f) - f_0}_{\infty}
		&= \sup_{x \in \calD} \abs{\reg_{\beta}f(x) - f_0(x)} \\
		&= \sup_{x \in \calD} \abs{f(x) \regindi - f_0(x)}.
	\end{align}
	Thus
	\begin{align}
		&\{\norm{\reg_{\beta}(f) - f_0}_{\infty} < r\}
		= \bigcap_{x \in \calD} \left\{ \abs{f(x) \regindi - f_0(x)} < r \right\} \\
		=& \left[ \{M_{\beta}(f) = \infty\} \cap \{\norm{f_0}_{\infty} < r\} \right] \nonumber \\
		 & \qquad \cup \left[ \{M_{\beta}(f) < \infty\} \cap \left( \cap_{x \in \calD} \{\abs{f(x) - f_0(x)} < r\} \right) \right].
	\end{align}
	The event $\{\norm{f_0}_{\infty} < r\}$ is an artifact of our construction and is always equal to $\emptyset$ or $\boxspace$, thus is measurable. Since $M_{\beta}$ is measurable, $\{M_{\beta}(f) < \infty\}$ and $\{M_{\beta}(f) = \infty\}$ is measurable. $\{\abs{f(x) - f_0(x)} < r\}$ depends only on pointwise evaluations of $f$ thus is measurable. Hence $\{\norm{\reg_{\beta}(f) - f_0} < r\}$ is measurable as a countable intesection and union of measurable sets.
	\end{subproof}

	Hence $\hat{F} = \reg_{\beta}(F)$ is a continuous stochastic process $\Omega \to C(\unitbox, \R)$. There's no point defining $\reg_{\beta}(F)$ if it doesn't carry over properties of $F$, so the first thing we'll do is show the two processes are modifications of each other. Rigirously we show that for all $x \in [0, 1]^d$, $\reg_{\beta}(F)_x = F_x$ almost surely\footnote{Note this means $\prob(\reg_{\beta}(F)_x = F_x) = 1$ for all $x$, not $\prob(\reg_{\beta}(F)_x = F_x \ \forall x) = 1$}. We do this is two steps, firstly for $x \in \calD$ and then for all $x \in [0, 1]^d$. When $x \in \calD$, $\reg_{\beta}(F)_x = F_x \indi_{\{M_{\beta}(F) < \infty\}}$. Thus it suffices to show $M_{\beta}(F) < \infty$ almost surely.
	\begin{claim}
		$M_{\beta}(F) < \infty$ almost surely.
	\end{claim}
	\begin{subproof}
		We can actually show the stronger statement $\E[M_{\beta}(F)] < \infty$. INSERT REST OF PROOF.
	\end{subproof}
	
	Now we extend this to all $x \in \unitbox$.
	\begin{claim}
		$\prob(\reg_{\beta}(F)_x = F_x) = 1$ for all $x \in [0, 1]^d$.
	\end{claim}
	\begin{subproof}
		For any $x \in \unitbox$, let $x_n \to x$ where $x_n$ is a sequence in $\calD$. Then by condition (2) on $C$
		\begin{align}
			\norm{F_{x_n} - F_x}_2^2
			&= \var(F_{x_n} - F_x) \\
			&= C(x_n, x_n) - 2C(x_n, x) + C(x, x) \\
			&\leq K \norm{x - x_n}^{\alpha} \xrightarrow{n \to \infty} 0.
		\end{align}
		Thus $F_{x_n} \to F_x$ in $L^2$. So there is some subsequence along which $F_x = \lim_n F_{x_{k_n}}$ almost surely. By continuity $\reg_{\beta}(F)_x = \lim_n \reg_{\beta}(F)_{x_{k_n}}$ always. Thus almost surely
		\begin{equation}
			\reg_{\beta}(F)_x = \lim_n \reg_{\beta}(F)_{x_{k_n}}
			= \lim_n F_{x_{k_n}} = F_x.
		\end{equation}
	\end{subproof}

	Let $\mu$ be the law of $\reg_{\beta}(F)$ (this makes $\mu$ a measure on $C(\unitbox, \R)$). 
	\begin{claim}
		$\mu$ satisfies the following properties.
		\begin{enumerate}
			\item $\mu$ is Gaussian.
			\item For all $x, y \in \unitbox$, $C_{\mu}(\delta_x, \delta_y) = C(x, y)$.
			\item $\mu(C^{\beta}(\unitbox, \R)) = 1$.
			\item $\mu$ is uniquely determined by $C$ and independent of the $\beta \in (0, \alpha)$ chosen in the construction.
		\end{enumerate}
	\end{claim}
	\begin{subproof}
		For any finite set $x_1, \ldots, x_n \in \unitbox$, almost surely
		\begin{equation}
			(\reg_{\beta}(F)_{x_1}, \ldots, \reg_{\beta}(F)_{x_n})
			= (F_{x_1}, \ldots, F_{x_n})
		\end{equation}
		thus is jointly Gaussian. Thus for all $\lambda \in \R^n$, $(\sum_{i=1}^n \lambda_i \delta_{x_i})_* \mu$ is Gaussian. In other words if we define
		\begin{equation}
			\calA \defeq \spn\{ \delta_x : x \in [0, 1]^d\}
		\end{equation}
		then $l_* \mu$ is Gaussian for all $l \in \calA$. Since $\calA$ is weak-$*$ dense in $C([0, 1]^d, \R)^*$ by \vref{prop:dirac-density}, $\mu$ is Gaussian by \vref{prop:weak-star-suff}.

		For all $x, y \in [0, 1]^d$ we have
		\begin{equation}
			C_{\mu}(\delta_x, \delta_y) = \E[\reg_{\beta}(F)_x \reg_{\beta}(F)_y]
			= \E[F_x F_y] = C(x, y).
		\end{equation}

		By construction, $\reg_{\beta}(F)$ is $\beta$-H\"older continuous thus $\mu(C^{\beta}(\unitbox, \R)) = 1$.

		Finally since $C$ determines $l_* \mu$ for all $l \in \calA$, $\mu$ is uniquely determined by \vref{prop:weak-star-suff}. In particular $C$ is independent of $\beta$ thus $\mu$ is independent of $\beta$.
	\end{subproof}

	This finishes the proof.
\end{proof}

\appendix

\chapter{Miscellaneous Theorems}

\dirac*

\begin{proof}
	Suppose $\calA$ were not dense in $C(\unitbox, \R)^*$, say $l_0$ is not in the weak-* closure of $\calA$. Then by the Hahn-Banach seperation, there exists a weak-* continuous linear functional $L: C(\unitbox, \R)^* \to \R$ that seperates the closed convex set $\bar{\calA}$ and the compact convex set $\{l_0\}$, i.e. there exists $\alpha \in \R$ such that
	\begin{equation}
		L(l) \leq \alpha \leq L(l_0) \quad \forall l \in \bar{\calA}.
	\end{equation}
	Then as $\calA$ is a vector space, for all $l \in \calA$ and $\lambda \in \R$ we have $\lambda L(l) = L(\lambda l) \leq \alpha$. Therefore $L(l) = 0$. Since $L$ is continuous w.r.t.\ the weak-$*$ topology there exists $f \in C(\unitbox, \R)$ such that $L(l) = l(f)$ for all $l \in \calB^*$. Then $f(x) = L(\delta_x) = 0$ for all $x \in \calD$. Thus $f \equiv 0$ by density of $\calD$ and continuity of $f$. Hence $L \equiv 0$, contradiction.
\end{proof}

\begin{prop}
	\label{prop:weak-convergence-gaussians}
	Let $\mu_n$ be a sequence of Gaussian laws on $\R$. Suppose $\mu_n \towk \mu$. Then $\mu$ is Gaussian.
\end{prop}
\begin{proof}
	Suppose each $\mu_n \sim N(\theta_n, \sigma_n)$. $(\mu_n)$ is a tight sequence since it is weakly convergent. By tightness there exists an bounded interval $[K, -K]$ such that for all $n$, $\mu_n([-K, K]) > \frac{1}{2}$. Then $\theta_n \in [-K, K]$ for all $n$ else over half the mass of some $\mu_n$ would lie outside $[-K, K]$. Similarly
	\begin{equation}
		\mu_n([-K, K]) = \int_{-K}^K \frac{1}{\sqrt{2 \pi \sigma^2}} e^{-\frac{1}{2\sigma^2}(x - \theta_n)^2} \dif x \leq K \sqrt{\frac{2}{\pi\sigma_n^2}}.
		\end{equation}
		Thus $(\sigma_n)$ must be bounded else the above quantity could be reduced below $\frac{1}{2}$. Hence there exist a subsequence along which $\theta = \lim_n \theta_{k_n}$ and $\sigma = \lim_n \sigma_{k_n}$ both exist. Then since $x \mapsto e^{i \xi x}$ is a continuous bounded function, by definition of weak convergence
	\begin{equation}
		\hat{\mu}(\xi) = \lim_n \hat{\mu_n}(\xi) = \lim_n \hat{\mu}_{k_n}(\xi) = \lim_n e^{i \theta_n \xi - \frac{1}{2} \sigma_n^2 \xi^2} = e^{i \theta \xi - \frac{1}{2} \sigma^2}.
	\end{equation}
	Since Fourier transforms uniquely determine distributions, $\mu \sim N(\theta, \sigma^2)$.
\end{proof}

\printbibliography

\end{document}
